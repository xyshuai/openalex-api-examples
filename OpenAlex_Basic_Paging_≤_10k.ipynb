{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2Ct+wRfwXRyjwq8koaT4R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xyshuai/openalex-api-demo/blob/main/OpenAlex_Basic_Paging_%E2%89%A4_10k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQLpd5-z8Jrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66dc07d2-a8ab-4888-9df2-82e7ab5c66bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted. Saving to: /content/drive/MyDrive/OpenAlex\n",
            "‚ñ∂Ô∏è Requesting page 1 ...\n",
            "   Page 1: 200 records, total=200\n",
            "‚ñ∂Ô∏è Requesting page 2 ...\n",
            "   Page 2: 200 records, total=400\n",
            "‚ñ∂Ô∏è Requesting page 3 ...\n",
            "   Page 3: 200 records, total=600\n",
            "‚ñ∂Ô∏è Requesting page 4 ...\n",
            "   Page 4: 200 records, total=800\n",
            "‚ñ∂Ô∏è Requesting page 5 ...\n",
            "   Page 5: 200 records, total=1000\n",
            "‚ñ∂Ô∏è Requesting page 6 ...\n",
            "   Page 6: 200 records, total=1200\n",
            "‚ñ∂Ô∏è Requesting page 7 ...\n",
            "   Page 7: 200 records, total=1400\n",
            "‚ñ∂Ô∏è Requesting page 8 ...\n",
            "   Page 8: 200 records, total=1600\n",
            "‚ñ∂Ô∏è Requesting page 9 ...\n",
            "   Page 9: 200 records, total=1800\n",
            "‚ñ∂Ô∏è Requesting page 10 ...\n",
            "   Page 10: 200 records, total=2000\n",
            "‚ñ∂Ô∏è Requesting page 11 ...\n",
            "   Page 11: 200 records, total=2200\n",
            "‚ñ∂Ô∏è Requesting page 12 ...\n",
            "   Page 12: 200 records, total=2400\n",
            "‚ñ∂Ô∏è Requesting page 13 ...\n",
            "   Page 13: 200 records, total=2600\n",
            "‚ñ∂Ô∏è Requesting page 14 ...\n",
            "   Page 14: 200 records, total=2800\n",
            "‚ñ∂Ô∏è Requesting page 15 ...\n",
            "   Page 15: 200 records, total=3000\n",
            "‚ñ∂Ô∏è Requesting page 16 ...\n",
            "   Page 16: 200 records, total=3200\n",
            "‚ñ∂Ô∏è Requesting page 17 ...\n",
            "   Page 17: 200 records, total=3400\n",
            "‚ñ∂Ô∏è Requesting page 18 ...\n",
            "   Page 18: 200 records, total=3600\n",
            "‚ñ∂Ô∏è Requesting page 19 ...\n",
            "   Page 19: 200 records, total=3800\n",
            "‚ñ∂Ô∏è Requesting page 20 ...\n",
            "   Page 20: 200 records, total=4000\n",
            "‚ñ∂Ô∏è Requesting page 21 ...\n",
            "   Page 21: 200 records, total=4200\n",
            "‚ñ∂Ô∏è Requesting page 22 ...\n",
            "   Page 22: 200 records, total=4400\n",
            "‚ñ∂Ô∏è Requesting page 23 ...\n",
            "   Page 23: 200 records, total=4600\n",
            "‚ñ∂Ô∏è Requesting page 24 ...\n",
            "   Page 24: 200 records, total=4800\n",
            "‚ñ∂Ô∏è Requesting page 25 ...\n",
            "   Page 25: 200 records, total=5000\n",
            "‚ñ∂Ô∏è Requesting page 26 ...\n",
            "   Page 26: 200 records, total=5200\n",
            "‚ñ∂Ô∏è Requesting page 27 ...\n",
            "   Page 27: 200 records, total=5400\n",
            "‚ñ∂Ô∏è Requesting page 28 ...\n",
            "   Page 28: 200 records, total=5600\n",
            "‚ñ∂Ô∏è Requesting page 29 ...\n",
            "   Page 29: 200 records, total=5800\n",
            "‚ñ∂Ô∏è Requesting page 30 ...\n",
            "   Page 30: 200 records, total=6000\n",
            "‚ñ∂Ô∏è Requesting page 31 ...\n",
            "   Page 31: 200 records, total=6200\n",
            "‚ñ∂Ô∏è Requesting page 32 ...\n",
            "   Page 32: 200 records, total=6400\n",
            "‚ñ∂Ô∏è Requesting page 33 ...\n",
            "   Page 33: 200 records, total=6600\n",
            "‚ñ∂Ô∏è Requesting page 34 ...\n",
            "   Page 34: 200 records, total=6800\n",
            "‚ñ∂Ô∏è Requesting page 35 ...\n",
            "   Page 35: 200 records, total=7000\n",
            "‚ñ∂Ô∏è Requesting page 36 ...\n",
            "   Page 36: 200 records, total=7200\n",
            "‚ñ∂Ô∏è Requesting page 37 ...\n",
            "   Page 37: 200 records, total=7400\n",
            "‚ñ∂Ô∏è Requesting page 38 ...\n",
            "   Page 38: 200 records, total=7600\n",
            "‚ñ∂Ô∏è Requesting page 39 ...\n",
            "   Page 39: 200 records, total=7800\n",
            "‚ñ∂Ô∏è Requesting page 40 ...\n",
            "   Page 40: 200 records, total=8000\n",
            "‚ñ∂Ô∏è Requesting page 41 ...\n",
            "   Page 41: 200 records, total=8200\n",
            "‚ñ∂Ô∏è Requesting page 42 ...\n",
            "   Page 42: 200 records, total=8400\n",
            "‚ñ∂Ô∏è Requesting page 43 ...\n",
            "   Page 43: 200 records, total=8600\n",
            "‚ñ∂Ô∏è Requesting page 44 ...\n",
            "   Page 44: 200 records, total=8800\n",
            "‚ñ∂Ô∏è Requesting page 45 ...\n",
            "   Page 45: 200 records, total=9000\n",
            "‚ñ∂Ô∏è Requesting page 46 ...\n",
            "   Page 46: 200 records, total=9200\n",
            "‚ñ∂Ô∏è Requesting page 47 ...\n",
            "   Page 47: 200 records, total=9400\n",
            "‚ñ∂Ô∏è Requesting page 48 ...\n",
            "   Page 48: 200 records, total=9600\n",
            "‚ñ∂Ô∏è Requesting page 49 ...\n",
            "   Page 49: 200 records, total=9800\n",
            "‚ñ∂Ô∏è Requesting page 50 ...\n",
            "   Page 50: 200 records, total=10000\n",
            "\n",
            "üì• Finished downloading. Total records collected: 10000\n",
            "\n",
            "‚úÖ Export completed: 10000 records written.\n",
            "üìÑ CSV file saved at: /content/drive/MyDrive/OpenAlex/openalex_basic_paging_full.csv\n"
          ]
        }
      ],
      "source": [
        "# === OpenAlex Basic Paging (up to 10,000 records) ===\n",
        "# - Uses classic page-based pagination: page=1,2,...,MAX_PAGES\n",
        "# - Collects results into a single CSV\n",
        "# - Designed to run in Google Colab, but also works in a normal Python environment\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import csv\n",
        "\n",
        "# 1) Try to mount Google Drive (if in Colab); if it fails, fall back to a local folder\n",
        "USE_DRIVE = False\n",
        "try:\n",
        "    from google.colab import drive, files  # type: ignore\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        USE_DRIVE = True\n",
        "        save_dir = \"/content/drive/MyDrive/OpenAlex\"\n",
        "        print(\"‚úÖ Google Drive mounted. Saving to:\", save_dir)\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Failed to mount Google Drive. Falling back to /content. Error:\", e)\n",
        "        save_dir = \"/content\"\n",
        "except Exception:\n",
        "    # Not running in Colab; save to the current working directory\n",
        "    save_dir = \".\"\n",
        "    print(\"‚ÑπÔ∏è Not running in Colab. Saving to the current directory.\")\n",
        "\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "out_csv = os.path.join(save_dir, \"openalex_basic_paging_full.csv\")\n",
        "\n",
        "# 2) OpenAlex API configuration (basic paging)\n",
        "BASE = \"https://api.openalex.org/works\"\n",
        "\n",
        "# Core filtering conditions\n",
        "# - open_access.is_oa:true  ‚Üí only open-access works\n",
        "# - authorships.countries:countries/my ‚Üí at least one author with country code \"MY\"\n",
        "# - publication_year:2023-2024 ‚Üí works published between 2023 and 2024\n",
        "# - type: article or review\n",
        "FILTERS = [\n",
        "    \"open_access.is_oa:true\",\n",
        "    \"authorships.countries:countries/my\",\n",
        "    \"publication_year:2023-2024\",\n",
        "    \"type:types/article|types/review\",\n",
        "]\n",
        "\n",
        "# Paging settings:\n",
        "# - per_page ‚â§ 200 (OpenAlex limit)\n",
        "# - MAX_PAGES ‚â§ 50 ‚Üí at most 10,000 results (200 * 50)\n",
        "PER_PAGE = 200\n",
        "MAX_PAGES = 50\n",
        "\n",
        "PARAMS_BASE = {\n",
        "    \"filter\": \",\".join(FILTERS),\n",
        "    \"sort\": \"cited_by_count:desc\",\n",
        "    \"per_page\": PER_PAGE,\n",
        "    \"mailto\": \"your.name@domain.com\",  # TODO: replace with your email\n",
        "}\n",
        "\n",
        "# ---------- Helper functions ----------\n",
        "\n",
        "def flatten_authors(authorships):\n",
        "    \"\"\"Return a simple ';'-separated string of author names.\"\"\"\n",
        "    if not authorships:\n",
        "        return \"\"\n",
        "    names = []\n",
        "    for a in authorships:\n",
        "        nm = (a.get(\"author\") or {}).get(\"display_name\")\n",
        "        if nm:\n",
        "            names.append(nm)\n",
        "    return \"; \".join(names)\n",
        "\n",
        "def flatten_authors_affiliations(authorships):\n",
        "    \"\"\"\n",
        "    Return a string such as:\n",
        "        'Author A (Affil1;Affil2); Author B (AffilX)'\n",
        "    If no affiliations exist, only the author name is shown.\n",
        "    \"\"\"\n",
        "    if not authorships:\n",
        "        return \"\"\n",
        "    parts = []\n",
        "    for a in authorships:\n",
        "        nm = (a.get(\"author\") or {}).get(\"display_name\") or \"\"\n",
        "        affs = []\n",
        "        for inst in (a.get(\"institutions\") or []):\n",
        "            dn = inst.get(\"display_name\") or \"\"\n",
        "            if dn:\n",
        "                affs.append(dn)\n",
        "        if nm:\n",
        "            if affs:\n",
        "                parts.append(f\"{nm} ({';'.join(affs)})\")\n",
        "            else:\n",
        "                parts.append(nm)\n",
        "    return \"; \".join(parts)\n",
        "\n",
        "def top_concepts(concepts, n=3):\n",
        "    \"\"\"\n",
        "    Return the top-n concepts by score as a ';'-separated string of concept names.\n",
        "    \"\"\"\n",
        "    if not concepts:\n",
        "        return \"\"\n",
        "    ranked = sorted(concepts, key=lambda c: c.get(\"score\", 0), reverse=True)[:n]\n",
        "    return \"; \".join([c.get(\"display_name\", \"\") for c in ranked if c.get(\"display_name\")])\n",
        "\n",
        "def topic_label(t):\n",
        "    \"\"\"Format one topic as 'Name (score=0.87)'.\"\"\"\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    name = t.get(\"display_name\", \"\")\n",
        "    sc = t.get(\"score\", None)\n",
        "    if sc is not None:\n",
        "        return f\"{name} (score={sc:.2f})\"\n",
        "    return name\n",
        "\n",
        "def collect_first_author_country_codes(authorships):\n",
        "    \"\"\"\n",
        "    Collect country codes for the first author:\n",
        "    - authorships[0].countries[]\n",
        "    - authorships[0].institutions[].country_code\n",
        "    \"\"\"\n",
        "    if not authorships:\n",
        "        return \"\"\n",
        "    a0 = authorships[0]\n",
        "    s = set()\n",
        "    for c in (a0.get(\"countries\") or []):\n",
        "        if c:\n",
        "            s.add(c)\n",
        "    for inst in (a0.get(\"institutions\") or []):\n",
        "        cc = inst.get(\"country_code\")\n",
        "        if cc:\n",
        "            s.add(cc)\n",
        "    return \";\".join(sorted(s)) if s else \"\"\n",
        "\n",
        "def collect_institution_country_codes(work):\n",
        "    \"\"\"\n",
        "    Collect country codes from the top-level institutions[] array.\n",
        "    \"\"\"\n",
        "    s = set()\n",
        "    for inst in (work.get(\"institutions\") or []):\n",
        "        cc = inst.get(\"country_code\")\n",
        "        if cc:\n",
        "            s.add(cc)\n",
        "    return \";\".join(sorted(s)) if s else \"\"\n",
        "\n",
        "def collect_corresponding_author_country_codes(authorships):\n",
        "    \"\"\"\n",
        "    Collect country codes for corresponding author(s):\n",
        "    - For each authorship with is_corresponding=True:\n",
        "      * authorships[i].countries[]\n",
        "      * authorships[i].institutions[].country_code\n",
        "    \"\"\"\n",
        "    if not authorships:\n",
        "        return \"\"\n",
        "    s = set()\n",
        "    for a in authorships:\n",
        "        if not a.get(\"is_corresponding\"):\n",
        "            continue\n",
        "        for c in (a.get(\"countries\") or []):\n",
        "            if c:\n",
        "                s.add(c)\n",
        "        for inst in (a.get(\"institutions\") or []):\n",
        "            cc = inst.get(\"country_code\")\n",
        "            if cc:\n",
        "                s.add(cc)\n",
        "    return \";\".join(sorted(s)) if s else \"\"\n",
        "\n",
        "def pick_source_info(work):\n",
        "    \"\"\"\n",
        "    Try to infer the journal/source information using:\n",
        "    1) host_venue\n",
        "    2) primary_location.source\n",
        "    3) locations[].source (first available)\n",
        "    Returns: (journal_name, issn_l)\n",
        "    \"\"\"\n",
        "    journal = \"\"\n",
        "    issn_l = \"\"\n",
        "\n",
        "    host = work.get(\"host_venue\") or {}\n",
        "    journal = host.get(\"display_name\") or \"\"\n",
        "    issn_l = host.get(\"issn_l\") or \"\"\n",
        "\n",
        "    if not journal:\n",
        "        pl = work.get(\"primary_location\") or {}\n",
        "        src = pl.get(\"source\") or {}\n",
        "        journal = journal or src.get(\"display_name\") or \"\"\n",
        "        issn_l = issn_l or src.get(\"issn_l\") or \"\"\n",
        "\n",
        "    if not journal:\n",
        "        for loc in (work.get(\"locations\") or []):\n",
        "            src = loc.get(\"source\") or {}\n",
        "            if src.get(\"display_name\"):\n",
        "                journal = journal or src.get(\"display_name\")\n",
        "                issn_l = issn_l or src.get(\"issn_l\") or \"\"\n",
        "                break\n",
        "\n",
        "    return journal or \"\", issn_l or \"\"\n",
        "\n",
        "def get_corresponding_authors(authorships):\n",
        "    \"\"\"Return a ';'-separated list of corresponding author names.\"\"\"\n",
        "    if not authorships:\n",
        "        return \"\"\n",
        "    names = []\n",
        "    for a in authorships:\n",
        "        if a.get(\"is_corresponding\"):\n",
        "            nm = (a.get(\"author\") or {}).get(\"display_name\")\n",
        "            if nm:\n",
        "                names.append(nm)\n",
        "    return \";\".join(names) if names else \"\"\n",
        "\n",
        "def get_fwci_and_percentile(work):\n",
        "    \"\"\"\n",
        "    Extract FWCI and citation-normalized percentile information.\n",
        "    Returns (fwci, citation_percentile, top_1pct_flag, top_10pct_flag)\n",
        "    \"\"\"\n",
        "    fwci = work.get(\"fwci\", \"\")\n",
        "    if not isinstance(fwci, (int, float, str)):\n",
        "        fwci = \"\"\n",
        "\n",
        "    cnp = work.get(\"citation_normalized_percentile\") or {}\n",
        "    perc = cnp.get(\"value\", None)\n",
        "    citation_percentile = f\"{perc:.6f}\" if isinstance(perc, (int, float)) else \"\"\n",
        "\n",
        "    top1 = cnp.get(\"is_in_top_1_percent\") or cnp.get(\"is_in_top1_percent\")\n",
        "    top10 = cnp.get(\"is_in_top_10_percent\") or cnp.get(\"is_in_top10_percent\")\n",
        "\n",
        "    citation_top_1pct = \"Yes\" if top1 else (\"No\" if top1 is not None else \"\")\n",
        "    citation_top_10pct = \"Yes\" if top10 else (\"No\" if top10 is not None else \"\")\n",
        "\n",
        "    return str(fwci), citation_percentile, citation_top_1pct, citation_top_10pct\n",
        "\n",
        "def parse_apc_list(work):\n",
        "    \"\"\"\n",
        "    Normalize the 'apc_list' field into a readable string.\n",
        "    Handles list/dict/scalar/None and returns something like:\n",
        "        '2000 USD; 1500 EUR'\n",
        "    \"\"\"\n",
        "    apc = work.get(\"apc_list\", None)\n",
        "    items = []\n",
        "\n",
        "    def norm_one(x):\n",
        "        if isinstance(x, dict):\n",
        "            val = x.get(\"value\", None)\n",
        "            cur = x.get(\"currency\", \"\")\n",
        "            if val is not None and cur:\n",
        "                return f\"{val} {cur}\"\n",
        "            if val is not None:\n",
        "                return str(val)\n",
        "            if cur:\n",
        "                return cur\n",
        "            return str(x)\n",
        "        if isinstance(x, (int, float)):\n",
        "            return str(x)\n",
        "        if isinstance(x, str):\n",
        "            return x.strip()\n",
        "        return str(x)\n",
        "\n",
        "    if isinstance(apc, list):\n",
        "        for it in apc:\n",
        "            s = norm_one(it)\n",
        "            if s:\n",
        "                items.append(s)\n",
        "    elif isinstance(apc, dict):\n",
        "        s = norm_one(apc)\n",
        "        if s:\n",
        "            items.append(s)\n",
        "    elif isinstance(apc, (str, int, float)):\n",
        "        s = norm_one(apc)\n",
        "        if s:\n",
        "            items.append(s)\n",
        "\n",
        "    return \"; \".join(items)\n",
        "\n",
        "def parse_sdg_labels(work):\n",
        "    \"\"\"\n",
        "    Extract SDG information from the correct field 'sustainable_development_goals'.\n",
        "    Returns a string like: 'SDG 3: Good health and well-being (0.95); SDG 4: Quality education (0.87)'\n",
        "    \"\"\"\n",
        "    # Use the correct SDG field namesÔºö 'sustainable_development_goals'\n",
        "    sdg_field = work.get(\"sustainable_development_goals\")\n",
        "\n",
        "    if not sdg_field:\n",
        "        return \"\"\n",
        "\n",
        "    labels = []\n",
        "\n",
        "    # OpenAlex returns a list, where each element contains an id, display_name, and score\n",
        "    if isinstance(sdg_field, list):\n",
        "        for item in sdg_field:\n",
        "            if isinstance(item, dict):\n",
        "                # Extract the SDG number (from the id URL, for example: \"https://metadata.un.org/sdg/3\" -> \"3\"Ôºâ\n",
        "                sdg_id = item.get(\"id\", \"\")\n",
        "                sdg_number = \"\"\n",
        "                if sdg_id:\n",
        "                    # Extract the last numeric part from the URL\n",
        "                    try:\n",
        "                        sdg_number = sdg_id.rstrip('/').split('/')[-1]\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                name = item.get(\"display_name\", \"\")\n",
        "                score = item.get(\"score\")\n",
        "\n",
        "                if name:\n",
        "                    # Combine into a complete formatÔºöSDG 3: Good health and well-being (0.95)\n",
        "                    if sdg_number:\n",
        "                        full_label = f\"SDG {sdg_number}: {name}\"\n",
        "                    else:\n",
        "                        full_label = name\n",
        "\n",
        "                    if score is not None:\n",
        "                        labels.append(f\"{full_label} ({score:.2f})\")\n",
        "                    else:\n",
        "                        labels.append(full_label)\n",
        "        if labels:\n",
        "            return \"; \".join(labels)\n",
        "\n",
        "    # If it's not in list format, try other processing methods\n",
        "    elif isinstance(sdg_field, dict):\n",
        "        sdg_id = sdg_field.get(\"id\", \"\")\n",
        "        sdg_number = \"\"\n",
        "        if sdg_id:\n",
        "            try:\n",
        "                sdg_number = sdg_id.rstrip('/').split('/')[-1]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        name = sdg_field.get(\"display_name\", \"\")\n",
        "        score = sdg_field.get(\"score\")\n",
        "\n",
        "        if name:\n",
        "            if sdg_number:\n",
        "                full_label = f\"SDG {sdg_number}: {name}\"\n",
        "            else:\n",
        "                full_label = name\n",
        "\n",
        "            if score is not None:\n",
        "                return f\"{full_label} ({score:.2f})\"\n",
        "            return full_label\n",
        "\n",
        "    elif isinstance(sdg_field, str):\n",
        "        return sdg_field.strip()\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "# 3) Download all pages (basic paging)\n",
        "all_results = []\n",
        "total_downloaded = 0\n",
        "\n",
        "for page in range(1, MAX_PAGES + 1):\n",
        "    params = PARAMS_BASE.copy()\n",
        "    params[\"page\"] = page\n",
        "\n",
        "    print(f\"‚ñ∂Ô∏è Requesting page {page} ...\")\n",
        "    resp = requests.get(BASE, params=params, timeout=60)\n",
        "    if resp.status_code != 200:\n",
        "        print(f\"‚ö†Ô∏è HTTP {resp.status_code} on page {page}: {resp.text[:200]}\")\n",
        "        break\n",
        "\n",
        "    payload = resp.json()\n",
        "    results = payload.get(\"results\", [])\n",
        "\n",
        "    if not results:\n",
        "        print(f\"‚úÖ No more results at page {page}. Stopping.\")\n",
        "        break\n",
        "\n",
        "    all_results.extend(results)\n",
        "    total_downloaded += len(results)\n",
        "    print(f\"   Page {page}: {len(results)} records, total={total_downloaded}\")\n",
        "\n",
        "    # If this page returned fewer than PER_PAGE results, it is probably the last page\n",
        "    if len(results) < PER_PAGE:\n",
        "        print(\"‚ÑπÔ∏è Fewer results than 'per_page'; likely reached the final page.\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nüì• Finished downloading. Total records collected: {len(all_results)}\")\n",
        "\n",
        "# 4) Write to CSV\n",
        "out_headers = [\n",
        "    \"openalex_id\", \"doi\", \"title\", \"year\", \"type\", \"language\",\n",
        "    \"cited_by_count\", \"journal\", \"issn_l\",\n",
        "    \"is_oa\", \"oa_status\", \"oa_url\", \"license\", \"version\",\n",
        "    \"first_author\", \"authors_affiliations\", \"top3_concepts\",\n",
        "    \"primary_topic_id\", \"primary_topic_name\",\n",
        "    \"primary_topic_domain\", \"primary_topic_field\", \"primary_topic_subfield\",\n",
        "    \"topics_top5\",\n",
        "    # Countries\n",
        "    \"first_author_country_codes\",\n",
        "    # Corresponding authors\n",
        "    \"corresponding_authors\",\n",
        "    \"corresponding_author_country_codes\",\n",
        "    # APC\n",
        "    \"apc_list_values\",\n",
        "    # Citation metrics\n",
        "    \"fwci\", \"citation_percentile\", \"citation_top_1pct\", \"citation_top_10pct\",\n",
        "    # SDG labels\n",
        "    \"sdg_labels\",\n",
        "]\n",
        "\n",
        "with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow(out_headers)\n",
        "\n",
        "    for wobj in all_results:\n",
        "        ids = wobj.get(\"ids\") or {}\n",
        "        doi = ids.get(\"doi\", \"\")\n",
        "        title = wobj.get(\"title\") or wobj.get(\"display_name\", \"\")\n",
        "        year = wobj.get(\"publication_year\", \"\")\n",
        "        wtype = wobj.get(\"type\", \"\")\n",
        "        lang = wobj.get(\"language\", \"\")\n",
        "        cited = wobj.get(\"cited_by_count\", 0)\n",
        "        journal, issn_l = pick_source_info(wobj)\n",
        "\n",
        "        oa = wobj.get(\"open_access\") or {}\n",
        "        is_oa = \"Yes\" if oa.get(\"is_oa\") else \"No\"\n",
        "        oa_status = oa.get(\"oa_status\", \"\")\n",
        "        oa_url = oa.get(\"oa_url\", \"\")\n",
        "\n",
        "        boa = wobj.get(\"best_oa_location\") or {}\n",
        "        license_ = boa.get(\"license\", \"\")\n",
        "        version = boa.get(\"version\", \"\")\n",
        "\n",
        "        authorships = wobj.get(\"authorships\", [])\n",
        "        first_author = authorships[0].get(\"author\", {}).get(\"display_name\") if authorships else \"\"\n",
        "        authors_aff = flatten_authors_affiliations(authorships)\n",
        "        concepts = wobj.get(\"concepts\", [])\n",
        "        top3 = top_concepts(concepts)\n",
        "\n",
        "        primary = wobj.get(\"primary_topic\") or {}\n",
        "        topics_list = wobj.get(\"topics\") or []\n",
        "        primary_topic_id = primary.get(\"id\", \"\")\n",
        "        primary_topic_name = primary.get(\"display_name\", \"\")\n",
        "        primary_topic_domain = (primary.get(\"domain\") or {}).get(\"display_name\", \"\")\n",
        "        primary_topic_field = (primary.get(\"field\") or {}).get(\"display_name\", \"\")\n",
        "        primary_topic_subfield = (primary.get(\"subfield\") or {}).get(\"display_name\", \"\")\n",
        "\n",
        "        others = [t for t in topics_list if (t.get(\"id\") != primary_topic_id)]\n",
        "        others_sorted = sorted(others, key=lambda t: t.get(\"score\", 0), reverse=True)[:5]\n",
        "        topics_top5 = \"; \".join([topic_label(t) for t in others_sorted])\n",
        "\n",
        "        first_author_cc = collect_first_author_country_codes(authorships)\n",
        "        corresponding_authors = get_corresponding_authors(authorships)\n",
        "        corr_author_cc = collect_corresponding_author_country_codes(authorships)\n",
        "        apc_list_values = parse_apc_list(wobj)\n",
        "        fwci, citation_percentile, top1, top10 = get_fwci_and_percentile(wobj)\n",
        "        sdg_labels = parse_sdg_labels(wobj)\n",
        "\n",
        "        w.writerow([\n",
        "            wobj.get(\"id\", \"\"), doi, title, year, wtype, lang,\n",
        "            cited, journal, issn_l,\n",
        "            is_oa, oa_status, oa_url, license_, version,\n",
        "            first_author, authors_aff, top3,\n",
        "            primary_topic_id, primary_topic_name,\n",
        "            primary_topic_domain, primary_topic_field, primary_topic_subfield,\n",
        "            topics_top5,\n",
        "            first_author_cc,\n",
        "            corresponding_authors,\n",
        "            corr_author_cc,\n",
        "            apc_list_values,\n",
        "            fwci, citation_percentile, top1, top10,\n",
        "            sdg_labels,\n",
        "        ])\n",
        "\n",
        "print(f\"\\n‚úÖ Export completed: {len(all_results)} records written.\")\n",
        "print(\"üìÑ CSV file saved at:\", out_csv)\n",
        "\n",
        "# Optional: if not using Drive in Colab, automatically trigger a download\n",
        "try:\n",
        "    if not USE_DRIVE:\n",
        "        from google.colab import files  # type: ignore\n",
        "        files.download(out_csv)\n",
        "except Exception:\n",
        "    pass"
      ]
    }
  ]
}